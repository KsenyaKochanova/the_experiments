>>> y_true = [2, 0, 2, 2, 0, 1]
>>> y_pred = [0, 0, 2, 2, 0, 2]
>>> confusion_matrix(y_true, y_pred)
array([[2, 0, 0],
       [0, 0, 1],
       [1, 0, 2]])


Cамый простой способ добраться до решения ли классификатор хорошо это или плохо, это просто вычислить ошибку, используя некоторые из стандартных мер ошибках (например, средний квадрат ошибки). Я полагаю, ваш пример копируется из документации Scikit, так что я предполагаю, что вы читали определение.
У нас есть три класса здесь: 0, 1 и 2. На диагонали, путаница матрица говорит вам, как часто конкретный класс были предсказаны правильно. Таким образом, с диагональю 2 0 2, мы можем сказать, что класс с индексом 0 был классифицирован правильно 2 раза, класс с индексом 1 не был предсказал правильно, и класс с индексом 2 было предсказано правильно в 2 раза.
Под и над диагональю вас есть номера, которые говорят вам, сколько раз класс с индексом, равным номеру строки элемента был классифицирован как класс с индексом, равным колонке матрицы. Например, если вы посмотрите на первой колонке, под диагональю у вас есть: 0 1 (в левом нижнем углу матрицы). Нижняя 1 говорит вам, что класс с индексом 2 (последний ряд) был когда-то ошибочно классифицируется как 0 (первая колонка). Это соответствует тому, что в вашем y_true был один образец с этикеткой 2 и классифицируются как 0. Это случилось в первом образце.
Если вы суммировать все цифры от матрицы неточностей Вы получаете ряд испытаний образцов (2 + 2 + 1 + 1 = 6 - равно длине y_true и y_pred). Если вы сложите строки вы получите количество образцов для каждой этикетки: как вы можете убедиться, в самом деле есть два 0s, один 1 и три 2s в y_pred.
Если вы, например, разделяй матричных элементов от этого числа, вы могли бы сказать, что, например, класс с меткой 2 признается с правильно с ~ 66% точностью, а в 1/3 случаев это путать (отсюда и название) с классом с Этикетка 0.
TL; DR:
В то время как меры об ошибках одного измерения количество общую производительность, с неразберихой матрицы можно определить, если (некоторые примеры):
Ваш классификатор просто отстой со всем
или он обрабатывает некоторые классы хорошо, и некоторые не (это дает вам подсказку, чтобы посмотреть на этого определенной части данных и наблюдать поведение классификаторов для этих случаев)
это хорошо, но смущает лейбл с B довольно часто. Например, для линейных классификаторов вы можете проверить то, если эти классы линейно разделимы.

Валидация – краеугольный камень машинного обучения. Это единственный рациональный способ оценить, насколько хорошо обобщает модель. Если исходный обучающий набор данных достаточно велик, его можно разделить на две части и использовать одну из них для обучения, другую – для валидации. Если же исходный обучающий набор данных мал, тогда не обойтись без кросс-валидации (cross-validation), более требовательной к ресурсам.
